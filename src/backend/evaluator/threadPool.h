#ifndef threadPool_h
#define threadPool_h

class ThreadPool {
public:
	explicit ThreadPool(size_t nthreads = (std::thread::hardware_concurrency() / 2))
		: stop(false)
	{
		if (nthreads == 0) nthreads = 1;
		workers.reserve(nthreads);
		for (size_t i = 0; i < nthreads; ++i) spawnOne();
	}

	~ThreadPool() {
		// Ensure all jobs have fully finished (not just claimed)
		waitForCompletion();
		stop.store(true, std::memory_order_relaxed);
		for (auto& w : workers) w->retire.store(true, std::memory_order_relaxed);
		cv.notify_all(); // wake any sleepers
		for (auto& w : workers) if (w->th.joinable()) w->th.join();
	}

	struct Job {
		void (*fn)(void*);
		void* arg;
	};

	// Load a new round that references the caller-owned jobs (no copy/move).
	void resetAndLoad(const std::vector<Job>& new_jobs) {
		{
			std::lock_guard lk(mtx);
			jobsRef = &new_jobs;
			next.store(0, std::memory_order_relaxed);
			completed.store(0, std::memory_order_relaxed);
			end.store(static_cast<uint32_t>(new_jobs.size()), std::memory_order_release);
			round.fetch_add(1, std::memory_order_release);
		}
		cv.notify_all();
	}

	void waitForEmpty() {
		while (true) {
			uint32_t n = next.load(std::memory_order_acquire);
			uint32_t e = end.load(std::memory_order_acquire);
			if (n >= e) break;
			std::this_thread::yield();
		}
	}

	void waitForCompletion() {
		while (true) {
			uint32_t c = completed.load(std::memory_order_acquire);
			uint32_t e = end.load(std::memory_order_acquire);
			if (c >= e) break;
			std::this_thread::yield();
		}
	}

	void resizeThreads(size_t new_count) {
		size_t cur = workers.size();
		new_count = std::max(new_count, size_t(1));
		if (new_count > cur) {
			size_t add = new_count - cur;
			workers.reserve(workers.size() + add);
			for (size_t i = 0; i < add; ++i) spawnOne();
			return;
		}
		if (new_count < cur) {
			size_t kill = cur - new_count;
			waitForCompletion();
			for (size_t i = 0; i < kill; ++i)
				workers[workers.size() - 1 - i]->retire.store(true, std::memory_order_relaxed);
			cv.notify_all(); // wake sleepers so they can retire
			for (size_t i = 0; i < kill; ++i) {
				auto idx = workers.size() - 1;
				auto& w = workers[idx];
				if (w->th.joinable()) w->th.join();
				workers.pop_back();
			}
		}
	}

	size_t threadCount() const { return workers.size(); }

private:
	struct Worker {
		std::thread th;
		std::atomic<bool> retire{false};
	};

	void spawnOne() {
		auto w = std::make_unique<Worker>();
		Worker* self = w.get();
		w->th = std::thread([this, self]{ workerLoop(self); });
		workers.emplace_back(std::move(w));
	}

	void workerLoop(Worker* self) {
		uint64_t local_round = round.load(std::memory_order_acquire);
		while (true) {
			// Claim an index.
			uint32_t i = next.fetch_add(1, std::memory_order_acq_rel);
			uint32_t e = end.load(std::memory_order_acquire);

			if (i < e) {
				// Safe because resetAndLoad keeps jobsRef stable for the round.
				const Job j = (*jobsRef)[i];
				j.fn(j.arg);
				completed.fetch_add(1, std::memory_order_acq_rel);
				continue;
			}

			std::unique_lock lk(mtx);
			cv.wait(lk, [this, self, &local_round]{
				return self->retire.load(std::memory_order_relaxed)
					   || stop.load(std::memory_order_relaxed)
					   || round.load(std::memory_order_acquire) != local_round;
			});
			if (self->retire.load(std::memory_order_relaxed) ||
				stop.load(std::memory_order_relaxed)) {
				return;
			}
			local_round = round.load(std::memory_order_acquire);
		}
	}

	std::vector<std::unique_ptr<Worker>> workers;
	const std::vector<Job>* jobsRef{nullptr}; // reference to current roundâ€™s jobs
	std::atomic<uint32_t> next{0};          // next index to claim
	std::atomic<uint32_t> end{0};           // jobsRef->size()
	std::atomic<uint32_t> completed{0};      // number of jobs fully executed
	std::atomic<bool> stop{false};          // global shutdown (idle-only)
	std::atomic<uint64_t> round{0};         // generation/epoch for cv wakeups

	std::mutex mtx;
	std::condition_variable cv;
};

#endif /* threadPool_h */
